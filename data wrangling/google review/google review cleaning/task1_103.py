# -*- coding: utf-8 -*-
"""task1_103.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBQU7JqI7I66w_X5pYns0f8blcARZEZW

<div class="alert alert-block alert-success">
    
# FIT5196 Task 1 in Assessment 1
#### Student Name: Anushka Jemima
#### Student ID: 33617457


#### Student Name: Shruthi Shashidhara Shastry
#### Student ID: 33684669
Date: 30-08-2024


Environment: Python

Libraries used:
* re (for regular expression, installed and imported)
* pandas (for data manipulation)
* os (for interactions with os)
* json (for handling json files)
* datetime(for datetime manipulations)
    
</div>

<div class="alert alert-block alert-danger">
    
## Table of Contents

</div>    

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Patent Files](#examine) <br>
[4. Loading and Parsing Files](#load) <br>
$\;\;\;\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>
$\;\;\;\;$[4.2. Reading Files](#Read) <br>
$\;\;\;\;$[4.3. Whatever else](#latin) <br>
[5. Writing to CSV/JSON File](#write) <br>
$\;\;\;\;$[5.1. Verification - using the sample files](#test_xml) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

-------------------------------------

<div class="alert alert-block alert-warning">

## 1.  Introduction  <a class="anchor" name="Intro"></a>
    
</div>

This assessment is extracting data from semi-sctuctured text files and excel files.The dataset contained 15 `.txt` files  and 1 excel file with 15 sheets which included various information about user reviews from various business. In particular we want to extract the data for each business the review that was left and the response if given by a business.

-------------------------------------

<div class="alert alert-block alert-warning">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>
 </div>

The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:

* **re:** to define and use regular expressions
* **pandas:** to manipulate and store the data
* **os:** to interact with the os for loading and reading files
* **json:** to store data into a json format
* **datetime:** to perfrom date time manipulations
"""

# Import required libraries
import re
import pandas as pd
import os
import json
from datetime import datetime

#Mount the goole drive , to access files stored your google drive
from google.colab import drive
drive.mount('/content/drive')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 3.  Examining Raw Data <a class="anchor" name="examine"></a>

 </div>

First we want to understand the format of the data in our txt files and excel files , this will help us set conditions to extract the data accurately.
"""

# Set the path for your txt files
txt = '/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/Student Data/student_group103/group103_0.txt'


# Open the file from the file path
with open(txt, 'r', encoding='utf-8') as txtfile:
  #Read all lines
    lines = txtfile.readlines()

    # Display the the first 10 lines
    print("First 10 lines of the TXT file:")
    for i, line in enumerate(lines[:10]):
        print(f"{i + 1}: {line.strip()}")
# File path for excel
excel = '/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/Student Data/student_group103/group103.xlsx'

# Read the Excel file into a DataFrame
df = pd.read_excel(excel)

# Display the first few lines (e.g., first 10 rows) of the DataFrame
print("First 10 rows of the Excel file:")
df.head(10)

"""Having examined the file content, the following observations were made:
From the first 10 lines of the txt file we can see that each gmap id which is the unique id of a business has multiple open (<>) and close (</>) record tags, these records have a user id , review , date time of the review , response , picture added with the review and the rating.They are enclosed between open and close tags . Similarly the excel file has it as individual columns.

-------------------------------------

<div class="alert alert-block alert-warning">

## 4.  Loading and Parsing Files <a class="anchor" name="load"></a>

</div>

In this section, the files are parsed and processed. First of all, appropriate regular expressions are defined to extract desired information when reading the files.Since we have tags and the words between the tags can have different case or can be of different combinations , such as the business ID can be gmap_id or gmapid or gmap id, we take this into consideration when defining our regular expressions.

-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.1. Defining Regular Expressions <a class="anchor" name="Reg_Exp"></a>

Defining correct regular expressions is crucial in extracting desired information from the text efficiently.
"""

#Create a disctionary to store the patterns for each of the fields you wnat to extract
patterns = {
    # Search for the opening tag < , gmap and then a closing tag extract text between this and the next opening tag</gmap...>
    'GmapID': re.compile(r'<[^>]*gmap[^>]*>(.*?)<[^>]*/[^>]*gmap[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , resp and then a closing tag extract text between this and the next opening tag</resp...>
    #Ensures if response is used inside the tags it is still picked up
    'Response': re.compile(r'<[^>]*resp[^>]*>(.*?)<[^>]*/[^>]*resp[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , pic and then a closing tag extract text between this and the next opening tag</pic...>
    'Pics': re.compile(r'<[^>]*pic[^>]*>(.*?)<[^>]*/[^>]*pic[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , rat and then a closing tag extract text between this and the next opening tag</rat...>
    'Rate': re.compile(r'<[^>]*rat[^>]*>(.*?)<[^>]*/[^>]*rat[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , name and then a closing tag extract text between this and the next opening tag</name...>
    'Name': re.compile(r'<[^>]*name[^>]*>(.*?)<[^>]*/[^>]*name[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , user and then a closing tag extract text between this and the next opening tag</user...>
    #Look for combinations that have user and the word id after it , but dont select combinations of user and name
    'UserId': re.compile(r'<[^>]*\s*user(?![_\s]*name\b)(?:\s*_id)?\s*[^>]*>(.*?)<[^>]*\s*\/\s*user(?:\s*_id)?\s*[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , review or text and then a closing tag extract text between this and the next opening tag</review or text...>
    'Review': re.compile(r'<[^>]*(?:review|text)[^>]*>(.*?)<[^>]*/(?:review|text)[^>]*>', re.DOTALL | re.IGNORECASE),
    # Search for the opening tag < , date or time and then a closing tag extract text between this and the next opening tag</date or time...>
    'Date': re.compile(r'<[^>]*(?:date|time)[^>]*>(.*?)<[^>]*/(?:date|time)[^>]*>', re.DOTALL | re.IGNORECASE)
}

"""These patterns are used in the next step when reading the files.

-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.2. Reading Files <a class="anchor" name="Read"></a>

In this step, all files are read and parsed.
"""

# Set the path where your txt files are saved
dir_path = '/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/Student Data/student_group103'
# Create a list to hold all the data betweene the open and close record tags
records = []
#Loop through the files in the path
for file in os.listdir(dir_path):
  #only select the txt files
  if file.endswith('.txt'):
    #add the txt file name to create a file path
    file_path = os.path.join(dir_path,file)
   #Read the file
    with open(file_path,'r',encoding ='utf-8') as txtfile:
      #Set a buffer to hold the lines till a close tag of record is found
      record_buffer =""
      #Set a flag to check if the values are still in the same record open and close tag
      in_record = False
      #Count to save the line count
      line_count = 0
      # For each line extracted from the txt file
      for line in txtfile:
        #Check if a single line has both and open tag and a close tag of the record
        if re.search(r'\s*<record>\s*', line, re.IGNORECASE) and re.search(r'\s*</record>\s*', line, re.IGNORECASE):
          #If we are already inside a record tag then apped the other lines of the record till you find a close record tag
          if in_record:
            # check if a close record tag is found
            pre_end = re.search(r'(.*?)</record>', line).group(1)
            #add all the text before the close tag to your current record
            record_buffer += pre_end
            # append this record to the buffer holding all records
            records.append([record_buffer])
            #initialize buffer to empty cause we have stored a full record
            record_buffer = ""
            in_record = False
          #if and open tag and close tag occur one after the other on a single line
          #store the full record to the records list
          full_record = re.search(r'<record>(.*?)</record>',line)
          if full_record:
            records.append([full_record.group(1)])
          #search for the close record tag and text after it
          post_start = re.search(r'</record>(.*)',line)
          # if the text after the close record tage has an open record tag and some text
          if post_start and '<record>' in post_start.group(1):
            # store it in the record buffer as a new record
            record_buffer = re.search(r'<record>(.*)',post_start.group(1)).group(1)
            in_record = True
        else:
          # if open and close record tags were not in the same line
          # check if the line has an open record tag
          if re.search(r'\s*<record>\s*', line, re.IGNORECASE):
            # store all data after the tag into the buffer
            record_buffer = re.search(r'<record>(.*)',line).group(1)
            in_record = True
          # if the line has close record tag
          elif re.search(r'\s*</record>\s*', line, re.IGNORECASE):
               #search if a closed record tag is in the line
                pre_end = re.search(r'(.*?)</record>',line).group(1)
                # the text before the close tag is added to the record buffer
                record_buffer += pre_end
                records.append([record_buffer])
                record_buffer = ""
                in_record = False
         #if the line has just text and no open tag or close tag
          elif in_record:
                # the just add it to the record buffer holding all the curremt record values
                record_buffer += line

print("First 10 records:")
for i, record in enumerate(records[:10]):
    print(f"Record {i + 1}: {record[0]}")

"""Let's take a look at the first ten elements of the lists generated. We can see that each record , with all the data between the tags being saved into the list.

Now we want to extract the individual data like the gmap_id the review text the date and store them as individual columns, using regex patterns.
"""

#Initialize a list to store your data
all_data =[]
#Loop through each record within the list of records
for record in records:
  #Select the first index that holds all the text
  record_str = record[0]
  #Create a dictionary
  data = {}
  #Loop through each key value pair fom the patterns define previously
  for key, pattern in patterns.items():
    # Search for the pattern which is the value for each key
    find = pattern.search(record_str)
    # Check any text is found
    if find :
      # Save it to the data disctionary
      data[key] = find.group(1)
    else :
      # Otherwise set it as empty , this can be used to check if all the values are being captured
      data[key] = ' '
  #Append this disctionary to the main all_data
  all_data.append(data)
#Convert the list to a data frame
df = pd.DataFrame(all_data)

#Loop through each column
for column in df.columns:
    if df[column].dtype == object:
      #Remove any new line characters from the ends of the data extracted
        df[column] = df[column].str.strip('\n')
# Print the first 10 rows of the DataFrame
print("First 10 rows of the DataFrame:")
df.head(10)

"""We now want to check if all the data was captured , this can be done by ensureing no ' ' values have been set for any of the rows."""

# Check for columns with ' ' as the value
empty_value_counts = (df == ' ').sum()

# Print columns that have an empty sapce
print("Number of rows with ' ' under each column:")
print(empty_value_counts)

"""We now move onto extracting data from the excel file. the excel file has 15 sheets with columns representing the user is , name , gmap id , review text , date , rating and pics."""

# Set the path to the excel file
ex_file_path = '/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/Student Data/student_group103/group103.xlsx'
# Define the column names
cols = ['user_id','name','time','rating','text','pics','resp','gmap_id']
#Create a list to hold all extracted rows
data =[]

#Load the excel file
with pd.ExcelFile(ex_file_path) as xls:
  #Loop through each sheet
  for sheet in xls.sheet_names:
    try:
      #Read each row of the sheet
      df_read = pd.read_excel(xls,sheet_name = sheet,usecols = cols)
      # Append each row which represents a single record to the list
      data.append(df_read)
    except ValueError as e:
      #This checks for any errors in reading the sheet
      print(f"Error {sheet}:{e}")
#Combine all the rows extracted from each sheet
combined_ex = pd.concat(data,ignore_index=True)
#Set column name as preferred
combined_ex.rename(columns={'user_id':'UserId',
                             'name':'Name',
                              'time': 'Date',
                            'rating': 'Rate',
                            'text':'Review',
                            'pics':'Pics',
                            'resp':'Response',
                            'gmap_id':'GmapID'},inplace=True)
#Combine the data extracted from txt files with that extracted from excel files
comp_data = pd.concat([df,combined_ex],ignore_index=True)

# Print the first 10 rows of the DataFrame
print("First 10 rows of the DataFrame:")
print(comp_data.head(10))

"""From above we notice an increase in the number of rows , this shows adding all the excel file rows to the txt file rows.

------------------------------------------------------------------------------

<div class="alert alert-block alert-info">
    
### 4.3. Clean the Data Frame<a class="anchor" name="latin"></a>

The data frame has all values , but may contain Nulls or dupliacted records , we need to check and perfrom the required action.
"""

#Check if the data frame has any row which has all columns as null
nulls = comp_data.isnull().sum()
print(nulls)
# Drop only those rows where no values are present under any column
comp_data = comp_data.dropna(how='all')
print(comp_data.dtypes)

#Remove any unnecessary space at the end or beginning of the value under a column
comp_data = comp_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Count of records before removing duplicates
record_count_d = len(comp_data)

# Check for duplicates
duplicate_count = comp_data.duplicated().sum()

# Drop any duplicate rows from the DataFrame, retaining only the first occurrence of the rows
comp_data = comp_data.drop_duplicates()

# Count of records after removing duplicates
record_count = len(comp_data)

# Display
print(f"Count of records before removing duplicates: {record_count_d}")
print(f"Count of duplicate rows: {duplicate_count}")
print(f"Count of records after removing duplicates: {record_count}")

"""We remove rows that have null values for the entire row , and drop duplicates ."""

#Ouputs from the excel file have Nan values we need to set them to None so that they match my txt file outputs
comp_data = comp_data.where(pd.notna(comp_data), None)

"""Now for the text processing we remove emoji and replace it with an empty space since its still a valid review."""

#Set a pattern for emoji
emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
flags=re.UNICODE)

#Create a function to remove emoji
def remove_emojis(text):
    #If the emoji pattern is found
    if isinstance(text, str):
      #Return an emtu space
        return emoji_pattern.sub(' ', text)
    else:
      #If not found the just return the text
        return text
#Apply the emoji remove function to my review data
comp_data['Review'] = comp_data['Review'].apply(remove_emojis)

"""Once we have perfromed all required cleaning we now want to create a CSV  with following columns:
* gmap_id: the ID of the business.
* review_count: the number of total reviews for a business.
* review_text_count: the number of reviews that contains a text.
* response_count: the number of responses from a business.


This gives us an aggregate of the business the number of reviews that are left and how many responses were left by the business.
"""

#Group all the gmap id's
data_csv = comp_data.groupby('GmapID').agg(
    #Get the count of reviews that are left
    review_count = pd.NamedAgg(column = 'Review',aggfunc ='size'),
    #Get the count of reviews that contain text , those that are not None
    review_text_count = pd.NamedAgg(column ='Review',aggfunc =lambda x:(x.notna()&(x != 'None')).sum()),
    #Get the count of the number of responses left that are not None
    response_count = pd.NamedAgg(column='Response',aggfunc=lambda x:(x.notna() &(x != 'None')).sum())
)
#Reset index from Gmap id to a new index
data_csv.reset_index(inplace=True)
#Rename columns as per requirement
data_csv.rename(columns={'GmapID': 'gmap_id'}, inplace=True)

# Print the first 10 rows of the DataFrame
print("First 10 rows of the DataFrame:")
data_csv.head(10)

#Convert the data frame to a csv and save it
data_csv.to_csv('task1_103.csv', index=False)
print(f"File saved in: {os.getcwd()}")

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 5.  Writing to JSON File <a class="anchor" name="write"></a>

</div>

Now we want to store all our data into a json file for easy loading and extracyion of data. We need to perfrom certain cleaning steps before loading the data to json.
"""

#This function will take the text of a review as input and perform some cleaning on it and return the text
#The cleaning perfromed is to save only english translations and not the original words
def process_text(text):
    #If the text has na or empty sapce set it to None
    if  pd.isna(text) or not text or text.isspace():
        return "none"
    #Set a regular expression to capture all the text the comes after the words (Transalted by Google)
    translated_regex = re.compile(r'\(Trans[a-z]* by Google\)', re.IGNORECASE)
    #Set a regular expression to capture all the text the comes after the words (Original)
    original_regex = re.compile(r'\(Orig[a-z]*\)', re.IGNORECASE)

    #Serach for the translated regular expression
    match_translated = translated_regex.search(text)
    #If the pattern is found
    if match_translated:
      #Get the index for the values that come after that words (Transalted by Google)
        start_index = match_translated.end()
        text = text[start_index:]
    #Search for the original pattenr
    match_original = original_regex.search(text)
    if match_original:
      #Set the enidng index to be where the (Original) ends
        text = text[:match_original.start()]
    #Remove any spaces and convert to lower
    text = text.strip().lower()
    return text if text else "none"

#The function pic_dimen takes the url as an input and extracts the dimensions h and d and saves it as a list of lists
def pic_dimen(url):
    #define pattern to find the h and d in the URL
    pattern = r"=w(\d+)-h(\d+)"
    #find the pattern
    match = re.search(pattern, url)
    if match:
      #if found save it as a list of list
        return [match.group(2), match.group(1)]
    return None

#The function pic_extract will take the entire text under the pic column , it can contain multiple url's and extracts the dimanesions of the pictures
def pic_extract(pics_str):
    #If an empty space of None is present then return and empty list
    if pd.isna(pics_str) or pics_str == "None":
        return []
    #replace all single quotes to double quotes , this is the expected format for a json
    pics_str_json = pics_str.replace("'", '"')
    #Load this list into a json , this is done to convert into dictionary with url as the key
    pics_list = json.loads(pics_str_json)
    # Count the number of keys which have the word url
    all_urls = sum([pic.get('url', []) for pic in pics_list], [])

    #extract the dimension from the url's
    pic_dims = list(filter(None, map(pic_dimen, all_urls)))

    return pic_dims

#This function takes individual reviews under a paticular gmap_id and alters the values to fit the JSON format that is expected
def transformation(Gmap_group):
  #Convert the date time stamp to the date time format
    Gmap_group['datetime'] = Gmap_group['Date'].apply(lambda x: datetime.utcfromtimestamp(float(x) / 1000.0))
    #Get the earliest review date under a gmap_id
    earliest_review_date = Gmap_group['datetime'].min().strftime('%Y-%m-%d %H:%M:%S')
    #Get the last review date under a gmap_id
    latest_review_date = Gmap_group['datetime'].max().strftime('%Y-%m-%d %H:%M:%S')
    #This creates a nested format for each review
    reviews =[{
      "user_id": row['UserId'],
      "time": datetime.utcfromtimestamp(float(row['Date'] )/ 1000.0).strftime('%Y-%m-%d %H:%M:%S'),
      "review_rating": row['Rate'],
      "review_text": process_text(row['Review']),
      #Set to Y or N oif there is a URL for the review
      "if_pic": "N" if pd.isna(row['Pics']) or row['Pics'] == "None" else "Y",
      #Display the picture dimensions as a list of lists
      "pic_dim": pic_extract(row['Pics']),
      "if_response" : "N" if pd.isna(row['Response']) or row['Response'] == "None" else "Y",

     } for index,row in Gmap_group.iterrows()]
    #Return the nested reviews for each gmap_id
    return{"reviews": reviews,
           "earliest_review_date": earliest_review_date,
              "latest_review_date": latest_review_date}

#Group the data by GMAP_ID since the json requires a Gmap_id and then nested with the reviews under it
gmaps_data = comp_data.groupby('GmapID')
json_output ={gmap_id: transformation(group)for gmap_id,group in gmaps_data}

#Write the json structured data to the output file
with open("task1_103.json", "w") as outfile:
         json.dump(json_output, outfile,indent=4)

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 6. Summary <a class="anchor" name="summary"></a>

</div>

In summary we have extracted raw google review data using regular expresssions, we have categorised them into individual columns in a data frame for ease of use and manipulations. We then get an aggregate of the reviews and responses under each business , and we store the data in an aggregated json format for ease of use.

-------------------------------------

<div class="alert alert-block alert-warning">

## 7. References <a class="anchor" name="Ref"></a>

</div>

[1]<a class="anchor" name="ref-2"></a> Why do I need to add DOTALL to python regular expression to match new line in raw string, https://stackoverflow.com/questions/22610247, Accessed 07/08/2024.

[2] How do I created nested JSON object with Python? https://stackoverflow.com/questions/52281671/how-do-i-created-nested-json-object-with-python Accessed 14/08/2024.

[3]Is it worth using Python's re.compile? https://stackoverflow.com/questions/452104/is-it-worth-using-pythons-re-compile
Accessed 14/08/2024.

## --------------------------------------------------------------------------------------------------------------------------
"""